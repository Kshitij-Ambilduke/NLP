{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "seq2seq.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPP9tXTrCOnQRQNLZNRxboe",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kshitij-Ambilduke/NLP/blob/master/seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwYnfrzPWj97"
      },
      "source": [
        "import torch\r\n",
        "from torchtext.datasets import Multi30k\r\n",
        "from torchtext.data import Field, BucketIterator\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.optim as optim\r\n",
        "import spacy\r\n",
        "import numpy as np\r\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kVKGGIFlQvV",
        "outputId": "e394ac46-1194-4099-b4b0-b48374c6a9a8"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtunrKIRWot3",
        "outputId": "cba41625-e90f-4320-e94b-232b8e0f2373"
      },
      "source": [
        "!python -m spacy download en --quiet\r\n",
        "!python -m spacy download de --quiet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "\u001b[K     |████████████████████████████████| 14.9MB 792kB/s \n",
            "\u001b[?25h  Building wheel for de-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/de_core_news_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/de\n",
            "You can now load the model via spacy.load('de')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cI-3240WWp16"
      },
      "source": [
        "spacy_german = spacy.load(\"de\")\r\n",
        "spacy_english = spacy.load(\"en\")\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZWwwnpLPl3N",
        "outputId": "259a2878-3833-49f3-dda2-bcc20a4b512b"
      },
      "source": [
        "vars(spacy_german)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'_meta': {'accuracy': {'ents_f': 82.998702191,\n",
              "   'ents_p': 83.4885634359,\n",
              "   'ents_r': 82.5145558435,\n",
              "   'las': 88.5644840323,\n",
              "   'tags_acc': 96.2969806963,\n",
              "   'token_acc': 95.8813352983,\n",
              "   'uas': 90.713095072},\n",
              "  'author': 'Explosion',\n",
              "  'description': 'German multi-task CNN trained on the TIGER and WikiNER corpus. Assigns context-specific token vectors, POS tags, dependency parse and named entities. Supports identification of PER, LOC, ORG and MISC entities.',\n",
              "  'email': 'contact@explosion.ai',\n",
              "  'factories': {'ner': 'ner', 'parser': 'parser', 'tagger': 'tagger'},\n",
              "  'labels': OrderedDict([('tagger',\n",
              "                ['$(',\n",
              "                 '$,',\n",
              "                 '$.',\n",
              "                 'ADJA',\n",
              "                 'ADJD',\n",
              "                 'ADV',\n",
              "                 'APPO',\n",
              "                 'APPR',\n",
              "                 'APPRART',\n",
              "                 'APZR',\n",
              "                 'ART',\n",
              "                 'CARD',\n",
              "                 'FM',\n",
              "                 'ITJ',\n",
              "                 'KOKOM',\n",
              "                 'KON',\n",
              "                 'KOUI',\n",
              "                 'KOUS',\n",
              "                 'NE',\n",
              "                 'NN',\n",
              "                 'NNE',\n",
              "                 'PAV',\n",
              "                 'PDAT',\n",
              "                 'PDS',\n",
              "                 'PIAT',\n",
              "                 'PIDAT',\n",
              "                 'PIS',\n",
              "                 'PPER',\n",
              "                 'PPOSAT',\n",
              "                 'PPOSS',\n",
              "                 'PRELAT',\n",
              "                 'PRELS',\n",
              "                 'PRF',\n",
              "                 'PROAV',\n",
              "                 'PTKA',\n",
              "                 'PTKANT',\n",
              "                 'PTKNEG',\n",
              "                 'PTKVZ',\n",
              "                 'PTKZU',\n",
              "                 'PWAT',\n",
              "                 'PWAV',\n",
              "                 'PWS',\n",
              "                 'TRUNC',\n",
              "                 'VAFIN',\n",
              "                 'VAIMP',\n",
              "                 'VAINF',\n",
              "                 'VAPP',\n",
              "                 'VMFIN',\n",
              "                 'VMINF',\n",
              "                 'VMPP',\n",
              "                 'VVFIN',\n",
              "                 'VVIMP',\n",
              "                 'VVINF',\n",
              "                 'VVIZU',\n",
              "                 'VVPP',\n",
              "                 'XY',\n",
              "                 '_SP']),\n",
              "               ('parser', []),\n",
              "               ('ner', [])]),\n",
              "  'lang': 'de',\n",
              "  'license': 'MIT',\n",
              "  'name': 'core_news_sm',\n",
              "  'notes': 'Because the model is trained on Wikipedia, it may perform inconsistently on many genres, such as social media text. The NER accuracy refers to the \"silver standard\" annotations in the WikiNER corpus. Accuracy on these annotations tends to be higher than correct human annotations.',\n",
              "  'parent_package': 'spacy',\n",
              "  'pipeline': ['tagger', 'parser', 'ner'],\n",
              "  'sources': [{'license': 'commercial (licensed by Explosion)',\n",
              "    'name': 'TIGER Corpus',\n",
              "    'url': 'https://www.ims.uni-stuttgart.de/forschung/ressourcen/korpora/tiger.html'},\n",
              "   {'license': 'CC BY 4.0',\n",
              "    'name': 'WikiNER',\n",
              "    'url': 'https://figshare.com/articles/Learning_multilingual_named_entity_recognition_from_Wikipedia/5462500'}],\n",
              "  'spacy_version': '>=2.2.2',\n",
              "  'speed': {'cpu': 6858.2108087967, 'gpu': None, 'nwords': 696811},\n",
              "  'url': 'https://explosion.ai',\n",
              "  'vectors': {'keys': 0, 'name': None, 'vectors': 0, 'width': 0},\n",
              "  'version': '2.2.5'},\n",
              " '_optimizer': None,\n",
              " '_path': PosixPath('/usr/local/lib/python3.6/dist-packages/spacy/data/de/de_core_news_sm-2.2.5'),\n",
              " 'max_length': 1000000,\n",
              " 'pipeline': [('tagger', <spacy.pipeline.pipes.Tagger at 0x7f0c338c0f98>),\n",
              "  ('parser', <spacy.pipeline.pipes.DependencyParser at 0x7f0c3319ab28>),\n",
              "  ('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x7f0c3319ab88>)],\n",
              " 'tokenizer': <spacy.tokenizer.Tokenizer at 0x7f0c33898390>,\n",
              " 'vocab': <spacy.vocab.Vocab at 0x7f0c338e3d48>}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rggpmKmvWrpw"
      },
      "source": [
        "def en_tokenizer(sen):\r\n",
        "    tokens = []\r\n",
        "    for token in spacy_english.tokenizer(sen):\r\n",
        "        tokens.append(token.text)\r\n",
        "    return tokens\r\n",
        "\r\n",
        "def de_tokenizer(sen):\r\n",
        "    tokens = []\r\n",
        "    for token in spacy_german.tokenizer(sen):\r\n",
        "        tokens.append(token.text)\r\n",
        "    return tokens\r\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDrFk0zLWtLv"
      },
      "source": [
        "SOURCE_Field = Field(eos_token = '<src_eos>', init_token = '<src_sos>', lower = True, tokenize = de_tokenizer)\r\n",
        "TARGET_Field = Field(eos_token = '<trg_eos>', init_token = '<trg_sos>', lower = True, tokenize = en_tokenizer)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrBa0w-IO_-J",
        "outputId": "ad5bcc1e-6d8e-4485-bfae-c58de1260e6d"
      },
      "source": [
        "vars(SOURCE_Field)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_first': False,\n",
              " 'dtype': torch.int64,\n",
              " 'eos_token': '<src_eos>',\n",
              " 'fix_length': None,\n",
              " 'include_lengths': False,\n",
              " 'init_token': '<src_sos>',\n",
              " 'is_target': False,\n",
              " 'lower': True,\n",
              " 'pad_first': False,\n",
              " 'pad_token': '<pad>',\n",
              " 'postprocessing': None,\n",
              " 'preprocessing': None,\n",
              " 'sequential': True,\n",
              " 'stop_words': None,\n",
              " 'tokenize': <function __main__.de_tokenizer>,\n",
              " 'truncate_first': False,\n",
              " 'unk_token': '<unk>',\n",
              " 'use_vocab': True,\n",
              " 'vocab': <torchtext.vocab.Vocab at 0x7f0c29ed60b8>}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7s2VFMLWv4o",
        "outputId": "c342fad7-3465-4165-a729-db9bd5049e98"
      },
      "source": [
        "train_data, valid_data, test_data = Multi30k.splits(exts = (\".de\", \".en\"),fields=(SOURCE_Field, TARGET_Field))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading training.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training.tar.gz: 100%|██████████| 1.21M/1.21M [00:02<00:00, 602kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading validation.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "validation.tar.gz: 100%|██████████| 46.3k/46.3k [00:00<00:00, 170kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading mmt_task1_test2016.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "mmt_task1_test2016.tar.gz: 100%|██████████| 66.2k/66.2k [00:00<00:00, 163kB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_De7ggLoPQIZ",
        "outputId": "eebdfddd-5ddb-40f0-90ff-41ed3425cf63"
      },
      "source": [
        "vars(test_data[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'src': ['ein',\n",
              "  'mann',\n",
              "  'mit',\n",
              "  'einem',\n",
              "  'orangefarbenen',\n",
              "  'hut',\n",
              "  ',',\n",
              "  'der',\n",
              "  'etwas',\n",
              "  'anstarrt',\n",
              "  '.'],\n",
              " 'trg': ['a',\n",
              "  'man',\n",
              "  'in',\n",
              "  'an',\n",
              "  'orange',\n",
              "  'hat',\n",
              "  'starring',\n",
              "  'at',\n",
              "  'something',\n",
              "  '.']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5J8JlnH1Wxuy"
      },
      "source": [
        "SOURCE_Field.build_vocab(train_data, min_freq=2)\r\n",
        "TARGET_Field.build_vocab(train_data, min_freq=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hVPBwxfWzbS",
        "outputId": "be5c8113-5b81-47c8-f9d3-18751a0f75a8"
      },
      "source": [
        "print(f\"Number of training examples: {len(train_data.examples)}\")\r\n",
        "print(f\"Number of validation examples: {len(valid_data.examples)}\")\r\n",
        "print(f\"Number of testing examples: {len(test_data.examples)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 29000\n",
            "Number of validation examples: 1014\n",
            "Number of testing examples: 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1dJ7bsTW04G",
        "outputId": "bb3090c7-fb26-49b3-a3ec-dd880324d969"
      },
      "source": [
        "print(vars(train_data.examples[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'src': ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.'], 'trg': ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRZO-In0W3NX"
      },
      "source": [
        "BATCH_SIZE = 128\r\n",
        "\r\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\r\n",
        "    (train_data, valid_data, test_data), \r\n",
        "    batch_size = BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXWjbsT2W481"
      },
      "source": [
        "class Encoder(nn.Module):\r\n",
        "    def __init__(self, source_vocab_len, embeddingsize, hiddensize, batchsize):\r\n",
        "        super().__init__()\r\n",
        "        self.batch_size = batchsize\r\n",
        "        self.embed = nn.Embedding(source_vocab_len, embeddingsize)\r\n",
        "        self.lstm = nn.LSTM(embeddingsize, hiddensize,num_layers=4)\r\n",
        "      \r\n",
        "    \r\n",
        "    def forward(self, x):\r\n",
        "        x = self.embed(x)              # input = (seqlen, batch), output = (seqlen, batch, embedding_dim)\r\n",
        "        op , (h,c) = self.lstm(x)      # input(x)=(seqlen,batch,embedding_dim), \r\n",
        "        h = torch.tanh(h)              # h = (1, batch, hidden_size)\r\n",
        "        c = torch.tanh(c)\r\n",
        "        return h,c"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRnFVBzUiAxZ"
      },
      "source": [
        "class Decoder(nn.Module):\r\n",
        "    def __init__(self, target_vocab_len, embeddingsize, hiddensize):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        self.opsize = target_vocab_len\r\n",
        "        self.embed = nn.Embedding(target_vocab_len,embeddingsize)\r\n",
        "        self.lstm = nn.LSTM(embeddingsize, hiddensize,num_layers=4)\r\n",
        "        self.fc = nn.Linear(hiddensize, self.opsize)\r\n",
        "        \r\n",
        "\r\n",
        "    def forward(self, x, h0, c0):\r\n",
        "\r\n",
        "        x = self.embed(x)                 # input    = [1, batch], output = [1, batch, embedding_dim]      \r\n",
        "        op, (h,c) = self.lstm(x,(h0,c0))  # input(x) = [1, batch, embedding_dim], op = [1, batch, hidden_dim] \r\n",
        "        op = torch.tanh(op)\r\n",
        "        op = self.fc(op)                  # op = [1, batch, vocabsize] (vocabsize==output_size)\r\n",
        "\r\n",
        "        return op,(h,c)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZY6VZKlPi5NT"
      },
      "source": [
        "class Translator(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self,encoder, decoder):\r\n",
        "        super().__init__()\r\n",
        "        self.encoder = encoder\r\n",
        "        self.decoder = decoder\r\n",
        "    \r\n",
        "    def forward(self, source, target):\r\n",
        "        \r\n",
        "        #source = [seq len, batch size]\r\n",
        "        #target = [seq len, batch size]\r\n",
        "\r\n",
        "        target_len = target.shape[0]\r\n",
        "        seq_len = target.shape[0]\r\n",
        "        vocab_len = self.decoder.opsize\r\n",
        "        batch_size = source.shape[-1]\r\n",
        "\r\n",
        "        hid_state, cell_state = self.encoder(source) \r\n",
        "\r\n",
        "        inp = target[0,:]                 #As input to the decoder is start token  #here, inp = [batch_size]\r\n",
        "        inp = inp.unsqueeze(0)                                                     #here, inp = [1,batch_size]\r\n",
        "        prediction = torch.zeros(target_len, batch_size, vocab_len) \r\n",
        "        for i in range(1, target_len):\r\n",
        "            batch_size = source.shape[-1]\r\n",
        "            output,state = self.decoder(inp, hid_state, cell_state) #output = [1, batch, vocabsize]\r\n",
        "            # print(output.shape)\r\n",
        "            hid_state, cell_state = state\r\n",
        "            prediction[i] = output.view(batch_size,self.decoder.opsize)\r\n",
        "            inp = target[i].unsqueeze(0)\r\n",
        "        # print(\"modelreturn=\",prediction.shape)            \r\n",
        "        return prediction\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3U3cSP1uoCRO"
      },
      "source": [
        "source_vocab_len = len(SOURCE_Field.vocab)\r\n",
        "target_vocab_len = len(TARGET_Field.vocab)\r\n",
        "encoder_embedding_dim = 64\r\n",
        "decoder_embedding_dim = 64\r\n",
        "hiddensize_encoder = 128\r\n",
        "hiddensize_decoder = 128\r\n",
        "enc = Encoder(source_vocab_len, encoder_embedding_dim, hiddensize_encoder, 128)\r\n",
        "dec = Decoder(target_vocab_len, decoder_embedding_dim, hiddensize_decoder )\r\n",
        "model = Translator(enc,dec)\r\n",
        "optimizer = optim.Adam(model.parameters(),lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPu9GGZuSRX3",
        "outputId": "1c665cb7-76a2-4a2c-c971-eb78fdd14deb"
      },
      "source": [
        "print(model)\r\n",
        "def count_parameters(model):\r\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n",
        "\r\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Translator(\n",
            "  (encoder): Encoder(\n",
            "    (embed): Embedding(7855, 64)\n",
            "    (lstm): LSTM(64, 128, num_layers=4)\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (embed): Embedding(5893, 64)\n",
            "    (lstm): LSTM(64, 128, num_layers=4)\n",
            "    (fc): Linear(in_features=128, out_features=5893, bias=True)\n",
            "  )\n",
            ")\n",
            "The model has 2,631,301 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzvBG6BFHaD8"
      },
      "source": [
        "#Our loss function calculates the average loss per token, however by passing the index of the <pad> token as \r\n",
        "#the ignore_index argument we ignore the loss whenever the target token is a padding token.\r\n",
        "target_padding_index = TARGET_Field.vocab.stoi[TARGET_Field.pad_token]\r\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = target_padding_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPWEtZPPHiCp"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\r\n",
        "  model.train()                               #just tells pytorch that we are in training phase\r\n",
        "  epoch_loss = 0\r\n",
        "  for i, batch in enumerate(train_iterator):\r\n",
        "      source = batch.src\r\n",
        "      target = batch.trg\r\n",
        "      # print(\"source=\",source.shape,\"target=\",target.shape)\r\n",
        "      # print(\"t=\",target.shape)\r\n",
        "      optimizer.zero_grad()\r\n",
        "      output = model.forward(source, target)  #target = [trg len, batch size]\r\n",
        "                                              #output = [trg len, batch size, output dim]\r\n",
        "      output_dim = output.shape[-1]\r\n",
        "      # print(\"op=\",output.shape)\r\n",
        "      output = output[1:].view(-1,output_dim)\r\n",
        "      target = target[1:].view(-1)\r\n",
        "      # print(\"op2=\",output.shape)\r\n",
        "      # print(\"t=\",target.shape)\r\n",
        "      loss = criterion(output, target)\r\n",
        "      \r\n",
        "      loss.backward()\r\n",
        "      optimizer.step()\r\n",
        "      epoch_loss += loss.item()\r\n",
        "\r\n",
        "  return epoch_loss / len(iterator)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKhdnaMhObjc",
        "outputId": "583f456b-0bca-436e-b972-4555775426f5"
      },
      "source": [
        "for epoch in range(10):\r\n",
        "    start = time.time()\r\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion)\r\n",
        "    end = time.time()\r\n",
        "    print(train_loss,end='|')\r\n",
        "    print(\" time taken = \",end-start )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.5261150152147605| time taken =  380.70202469825745\n",
            "3.4479502476259474| time taken =  380.0669410228729\n",
            "3.3693812019499387| time taken =  388.0589597225189\n",
            "3.293200131554961| time taken =  383.5241780281067\n",
            "3.2251068547958845| time taken =  392.76274013519287\n",
            "3.16307433271198| time taken =  384.28492760658264\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOy9zMbCHD5E"
      },
      "source": [
        "# def ipTensor(sentence, source_field):\r\n",
        "#       tokens = [source_field.init_token]+de_tokenizer(sentence)+[source_field.eos_token]\r\n",
        "#       seq_len = len(tokens)\r\n",
        "#       ip_tensor = torch.LongTensor([src_field.vocab.stoi[token]] for token in tokens) #words converted into numeric values here\r\n",
        "#       return ip_tensor.view(seq_len,1)\r\n",
        "\r\n",
        "def ipTensor(sentence, src_field):\r\n",
        "    if isinstance(sentence, list):\r\n",
        "        tokens = [src_field.init_token] + [token.lower() for token in sentence] + [src_field.eos_token]\r\n",
        "    else:\r\n",
        "        tokens = [src_field.init_token] + de_tokenizer(sentence) + [src_field.eos_token]\r\n",
        "    seq_len = len(tokens)\r\n",
        "    ip_tensor = torch.LongTensor([src_field.vocab.stoi[token] for token in tokens])\r\n",
        "    return ip_tensor.view(seq_len, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8K8ECjcUL6lM"
      },
      "source": [
        "def converter(source_sen, source_field, target_field, model):\r\n",
        "    input_tensor = ipTensor(source_sen, source_field)\r\n",
        "    with torch.no_grad():\r\n",
        "        states = model.encoder(input_tensor)\r\n",
        "    sos_loc = target_field.vocab.stoi[target_field.init_token]\r\n",
        "    eos_loc = target_field.vocab.stoi[target_field.eos_token]\r\n",
        "    predicts = [sos_loc]\r\n",
        "    sen_len =1\r\n",
        "    while sen_len < 50:\r\n",
        "        inp = torch.LongTensor([predicts[-1]]).view(1,-1)\r\n",
        "        with torch.no_grad():\r\n",
        "            h,c=states\r\n",
        "            output, states = model.decoder(inp, h,c)\r\n",
        "        output = output.squeeze()\r\n",
        "        output = output.view(-1,model.decoder.opsize)\r\n",
        "        predicts.append(output.argmax(-1).item())\r\n",
        "        sen_len+=1\r\n",
        "        if predicts[-1]==eos_loc:\r\n",
        "            break\r\n",
        "    sentence = [target_field.vocab.itos[it] for it in predicts[1:]]\r\n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "id": "CbgGjT-ySvuq",
        "outputId": "df42f98a-d301-4869-b49e-28a27074fe33"
      },
      "source": [
        "sen = ['ein', 'einzelner', 'mann', 'steht', 'abends', 'auf', 'einer', 'brücke', '.']\r\n",
        "output = converter(sen, SOURCE_Field, TARGET_Field, model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-22bafc55f6aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'ein'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'einzelner'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mann'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'steht'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'abends'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'auf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'einer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'brücke'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSOURCE_Field\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTARGET_Field\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'SOURCE_Field' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvcgheXBxOi9",
        "outputId": "1789ef83-5e31-4ed7-d9e2-f16aad654d86"
      },
      "source": [
        "print(output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['a', 'man', 'is', 'standing', 'on', 'the', 'sidewalk', 'next', 'to', 'a', 'woman', '.', '<trg_eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-lJGjGrgN_Z",
        "outputId": "5cb8f122-e6f2-4e94-8bf1-aacdc0985d03"
      },
      "source": [
        "print(output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['two', 'men', 'in', 'a', 'blue', 'shirt', 'and', 'a', 'woman', 'are', 'sitting', 'on', 'a', 'bench', '.', '<trg_eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxJM5F0lUCZy",
        "outputId": "bc896357-9194-408f-cdb0-7cb277e94689"
      },
      "source": [
        "print(output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['a', 'man', 'in', 'a', 'blue', 'shirt', 'is', 'standing', 'on', 'a', 'bench', '.', '<trg_eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L16TKWbqO2Gu",
        "outputId": "60e50c8d-1561-451d-8691-68992eeb8a57"
      },
      "source": [
        "for i in train_iterator:\r\n",
        "    print(i.src.shape)\r\n",
        "    print(i.trg.shape)\r\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([28, 128])\n",
            "torch.Size([28, 128])\n",
            "\n",
            "torch.Size([24, 128])\n",
            "torch.Size([25, 128])\n",
            "\n",
            "torch.Size([29, 128])\n",
            "torch.Size([34, 128])\n",
            "\n",
            "torch.Size([33, 128])\n",
            "torch.Size([35, 128])\n",
            "\n",
            "torch.Size([30, 128])\n",
            "torch.Size([28, 128])\n",
            "\n",
            "torch.Size([31, 128])\n",
            "torch.Size([27, 128])\n",
            "\n",
            "torch.Size([46, 128])\n",
            "torch.Size([37, 128])\n",
            "\n",
            "torch.Size([26, 128])\n",
            "torch.Size([27, 128])\n",
            "\n",
            "torch.Size([32, 128])\n",
            "torch.Size([32, 128])\n",
            "\n",
            "torch.Size([30, 128])\n",
            "torch.Size([26, 128])\n",
            "\n",
            "torch.Size([34, 128])\n",
            "torch.Size([35, 128])\n",
            "\n",
            "torch.Size([33, 128])\n",
            "torch.Size([35, 128])\n",
            "\n",
            "torch.Size([24, 128])\n",
            "torch.Size([28, 128])\n",
            "\n",
            "torch.Size([26, 128])\n",
            "torch.Size([34, 128])\n",
            "\n",
            "torch.Size([26, 128])\n",
            "torch.Size([27, 128])\n",
            "\n",
            "torch.Size([29, 128])\n",
            "torch.Size([33, 128])\n",
            "\n",
            "torch.Size([26, 128])\n",
            "torch.Size([25, 128])\n",
            "\n",
            "torch.Size([33, 128])\n",
            "torch.Size([29, 128])\n",
            "\n",
            "torch.Size([29, 128])\n",
            "torch.Size([28, 128])\n",
            "\n",
            "torch.Size([36, 128])\n",
            "torch.Size([37, 128])\n",
            "\n",
            "torch.Size([30, 128])\n",
            "torch.Size([27, 128])\n",
            "\n",
            "torch.Size([33, 128])\n",
            "torch.Size([27, 128])\n",
            "\n",
            "torch.Size([32, 128])\n",
            "torch.Size([32, 128])\n",
            "\n",
            "torch.Size([29, 128])\n",
            "torch.Size([26, 128])\n",
            "\n",
            "torch.Size([29, 128])\n",
            "torch.Size([37, 128])\n",
            "\n",
            "torch.Size([30, 128])\n",
            "torch.Size([31, 128])\n",
            "\n",
            "torch.Size([28, 128])\n",
            "torch.Size([29, 128])\n",
            "\n",
            "torch.Size([27, 128])\n",
            "torch.Size([33, 128])\n",
            "\n",
            "torch.Size([28, 128])\n",
            "torch.Size([30, 128])\n",
            "\n",
            "torch.Size([31, 128])\n",
            "torch.Size([32, 128])\n",
            "\n",
            "torch.Size([27, 128])\n",
            "torch.Size([27, 128])\n",
            "\n",
            "torch.Size([33, 128])\n",
            "torch.Size([36, 128])\n",
            "\n",
            "torch.Size([27, 128])\n",
            "torch.Size([34, 128])\n",
            "\n",
            "torch.Size([35, 128])\n",
            "torch.Size([35, 128])\n",
            "\n",
            "torch.Size([32, 128])\n",
            "torch.Size([37, 128])\n",
            "\n",
            "torch.Size([30, 128])\n",
            "torch.Size([29, 128])\n",
            "\n",
            "torch.Size([28, 128])\n",
            "torch.Size([26, 128])\n",
            "\n",
            "torch.Size([31, 128])\n",
            "torch.Size([31, 128])\n",
            "\n",
            "torch.Size([30, 128])\n",
            "torch.Size([33, 128])\n",
            "\n",
            "torch.Size([25, 128])\n",
            "torch.Size([27, 128])\n",
            "\n",
            "torch.Size([27, 128])\n",
            "torch.Size([28, 128])\n",
            "\n",
            "torch.Size([32, 128])\n",
            "torch.Size([25, 128])\n",
            "\n",
            "torch.Size([31, 128])\n",
            "torch.Size([28, 128])\n",
            "\n",
            "torch.Size([25, 128])\n",
            "torch.Size([28, 128])\n",
            "\n",
            "torch.Size([29, 128])\n",
            "torch.Size([26, 128])\n",
            "\n",
            "torch.Size([27, 128])\n",
            "torch.Size([30, 128])\n",
            "\n",
            "torch.Size([34, 128])\n",
            "torch.Size([41, 128])\n",
            "\n",
            "torch.Size([30, 128])\n",
            "torch.Size([32, 128])\n",
            "\n",
            "torch.Size([33, 128])\n",
            "torch.Size([35, 128])\n",
            "\n",
            "torch.Size([27, 128])\n",
            "torch.Size([27, 128])\n",
            "\n",
            "torch.Size([23, 128])\n",
            "torch.Size([27, 128])\n",
            "\n",
            "torch.Size([28, 128])\n",
            "torch.Size([38, 128])\n",
            "\n",
            "torch.Size([26, 128])\n",
            "torch.Size([30, 128])\n",
            "\n",
            "torch.Size([28, 128])\n",
            "torch.Size([27, 128])\n",
            "\n",
            "torch.Size([45, 128])\n",
            "torch.Size([42, 128])\n",
            "\n",
            "torch.Size([29, 128])\n",
            "torch.Size([30, 128])\n",
            "\n",
            "torch.Size([33, 128])\n",
            "torch.Size([34, 128])\n",
            "\n",
            "torch.Size([37, 128])\n",
            "torch.Size([36, 128])\n",
            "\n",
            "torch.Size([26, 128])\n",
            "torch.Size([29, 128])\n",
            "\n",
            "torch.Size([28, 128])\n",
            "torch.Size([28, 128])\n",
            "\n",
            "torch.Size([35, 128])\n",
            "torch.Size([37, 128])\n",
            "\n",
            "torch.Size([36, 128])\n",
            "torch.Size([38, 128])\n",
            "\n",
            "torch.Size([27, 128])\n",
            "torch.Size([27, 128])\n",
            "\n",
            "torch.Size([30, 128])\n",
            "torch.Size([32, 128])\n",
            "\n",
            "torch.Size([28, 128])\n",
            "torch.Size([28, 128])\n",
            "\n",
            "torch.Size([32, 128])\n",
            "torch.Size([27, 128])\n",
            "\n",
            "torch.Size([24, 128])\n",
            "torch.Size([24, 128])\n",
            "\n",
            "torch.Size([31, 128])\n",
            "torch.Size([29, 128])\n",
            "\n",
            "torch.Size([25, 128])\n",
            "torch.Size([25, 128])\n",
            "\n",
            "torch.Size([31, 128])\n",
            "torch.Size([28, 128])\n",
            "\n",
            "torch.Size([38, 128])\n",
            "torch.Size([37, 128])\n",
            "\n",
            "torch.Size([28, 128])\n",
            "torch.Size([31, 128])\n",
            "\n",
            "torch.Size([30, 128])\n",
            "torch.Size([29, 128])\n",
            "\n",
            "torch.Size([34, 128])\n",
            "torch.Size([31, 128])\n",
            "\n",
            "torch.Size([28, 128])\n",
            "torch.Size([28, 128])\n",
            "\n",
            "torch.Size([29, 128])\n",
            "torch.Size([27, 128])\n",
            "\n",
            "torch.Size([41, 128])\n",
            "torch.Size([42, 128])\n",
            "\n",
            "torch.Size([35, 128])\n",
            "torch.Size([40, 128])\n",
            "\n",
            "torch.Size([29, 128])\n",
            "torch.Size([29, 128])\n",
            "\n",
            "torch.Size([33, 128])\n",
            "torch.Size([30, 128])\n",
            "\n",
            "torch.Size([31, 128])\n",
            "torch.Size([29, 128])\n",
            "\n",
            "torch.Size([29, 128])\n",
            "torch.Size([30, 128])\n",
            "\n",
            "torch.Size([30, 128])\n",
            "torch.Size([33, 128])\n",
            "\n",
            "torch.Size([32, 128])\n",
            "torch.Size([32, 128])\n",
            "\n",
            "torch.Size([27, 128])\n",
            "torch.Size([28, 128])\n",
            "\n",
            "torch.Size([29, 128])\n",
            "torch.Size([29, 128])\n",
            "\n",
            "torch.Size([24, 128])\n",
            "torch.Size([24, 128])\n",
            "\n",
            "torch.Size([30, 128])\n",
            "torch.Size([34, 128])\n",
            "\n",
            "torch.Size([36, 128])\n",
            "torch.Size([28, 128])\n",
            "\n",
            "torch.Size([34, 128])\n",
            "torch.Size([34, 128])\n",
            "\n",
            "torch.Size([27, 128])\n",
            "torch.Size([26, 128])\n",
            "\n",
            "torch.Size([32, 128])\n",
            "torch.Size([27, 128])\n",
            "\n",
            "torch.Size([26, 128])\n",
            "torch.Size([31, 128])\n",
            "\n",
            "torch.Size([31, 128])\n",
            "torch.Size([29, 128])\n",
            "\n",
            "torch.Size([31, 128])\n",
            "torch.Size([31, 128])\n",
            "\n",
            "torch.Size([28, 128])\n",
            "torch.Size([32, 128])\n",
            "\n",
            "torch.Size([28, 128])\n",
            "torch.Size([29, 128])\n",
            "\n",
            "torch.Size([29, 128])\n",
            "torch.Size([27, 128])\n",
            "\n",
            "torch.Size([27, 128])\n",
            "torch.Size([30, 128])\n",
            "\n",
            "torch.Size([31, 128])\n",
            "torch.Size([36, 128])\n",
            "\n",
            "torch.Size([38, 128])\n",
            "torch.Size([37, 128])\n",
            "\n",
            "torch.Size([32, 128])\n",
            "torch.Size([26, 128])\n",
            "\n",
            "torch.Size([29, 128])\n",
            "torch.Size([33, 128])\n",
            "\n",
            "torch.Size([30, 128])\n",
            "torch.Size([27, 128])\n",
            "\n",
            "torch.Size([29, 128])\n",
            "torch.Size([32, 128])\n",
            "\n",
            "torch.Size([24, 128])\n",
            "torch.Size([28, 128])\n",
            "\n",
            "torch.Size([28, 128])\n",
            "torch.Size([30, 128])\n",
            "\n",
            "torch.Size([24, 128])\n",
            "torch.Size([25, 128])\n",
            "\n",
            "torch.Size([30, 128])\n",
            "torch.Size([30, 128])\n",
            "\n",
            "torch.Size([31, 128])\n",
            "torch.Size([30, 128])\n",
            "\n",
            "torch.Size([26, 128])\n",
            "torch.Size([29, 128])\n",
            "\n",
            "torch.Size([27, 128])\n",
            "torch.Size([27, 128])\n",
            "\n",
            "torch.Size([28, 128])\n",
            "torch.Size([32, 128])\n",
            "\n",
            "torch.Size([29, 128])\n",
            "torch.Size([31, 128])\n",
            "\n",
            "torch.Size([31, 128])\n",
            "torch.Size([30, 128])\n",
            "\n",
            "torch.Size([26, 128])\n",
            "torch.Size([24, 128])\n",
            "\n",
            "torch.Size([28, 128])\n",
            "torch.Size([29, 128])\n",
            "\n",
            "torch.Size([28, 128])\n",
            "torch.Size([26, 128])\n",
            "\n",
            "torch.Size([25, 128])\n",
            "torch.Size([24, 128])\n",
            "\n",
            "torch.Size([36, 128])\n",
            "torch.Size([43, 128])\n",
            "\n",
            "torch.Size([26, 128])\n",
            "torch.Size([26, 128])\n",
            "\n",
            "torch.Size([25, 128])\n",
            "torch.Size([26, 128])\n",
            "\n",
            "torch.Size([36, 128])\n",
            "torch.Size([39, 128])\n",
            "\n",
            "torch.Size([28, 128])\n",
            "torch.Size([30, 128])\n",
            "\n",
            "torch.Size([33, 128])\n",
            "torch.Size([30, 128])\n",
            "\n",
            "torch.Size([28, 128])\n",
            "torch.Size([27, 128])\n",
            "\n",
            "torch.Size([27, 128])\n",
            "torch.Size([30, 128])\n",
            "\n",
            "torch.Size([28, 128])\n",
            "torch.Size([29, 128])\n",
            "\n",
            "torch.Size([32, 128])\n",
            "torch.Size([27, 128])\n",
            "\n",
            "torch.Size([26, 128])\n",
            "torch.Size([25, 128])\n",
            "\n",
            "torch.Size([37, 128])\n",
            "torch.Size([28, 128])\n",
            "\n",
            "torch.Size([32, 128])\n",
            "torch.Size([34, 128])\n",
            "\n",
            "torch.Size([31, 128])\n",
            "torch.Size([35, 128])\n",
            "\n",
            "torch.Size([33, 128])\n",
            "torch.Size([35, 128])\n",
            "\n",
            "torch.Size([31, 128])\n",
            "torch.Size([30, 128])\n",
            "\n",
            "torch.Size([29, 128])\n",
            "torch.Size([27, 128])\n",
            "\n",
            "torch.Size([28, 128])\n",
            "torch.Size([32, 128])\n",
            "\n",
            "torch.Size([29, 128])\n",
            "torch.Size([38, 128])\n",
            "\n",
            "torch.Size([38, 128])\n",
            "torch.Size([37, 128])\n",
            "\n",
            "torch.Size([25, 128])\n",
            "torch.Size([25, 128])\n",
            "\n",
            "torch.Size([34, 128])\n",
            "torch.Size([32, 128])\n",
            "\n",
            "torch.Size([30, 128])\n",
            "torch.Size([30, 128])\n",
            "\n",
            "torch.Size([27, 128])\n",
            "torch.Size([31, 128])\n",
            "\n",
            "torch.Size([33, 128])\n",
            "torch.Size([29, 128])\n",
            "\n",
            "torch.Size([32, 128])\n",
            "torch.Size([29, 128])\n",
            "\n",
            "torch.Size([36, 128])\n",
            "torch.Size([31, 128])\n",
            "\n",
            "torch.Size([23, 128])\n",
            "torch.Size([28, 128])\n",
            "\n",
            "torch.Size([24, 128])\n",
            "torch.Size([26, 128])\n",
            "\n",
            "torch.Size([29, 128])\n",
            "torch.Size([30, 128])\n",
            "\n",
            "torch.Size([37, 128])\n",
            "torch.Size([40, 128])\n",
            "\n",
            "torch.Size([28, 128])\n",
            "torch.Size([29, 128])\n",
            "\n",
            "torch.Size([27, 128])\n",
            "torch.Size([30, 128])\n",
            "\n",
            "torch.Size([27, 128])\n",
            "torch.Size([31, 128])\n",
            "\n",
            "torch.Size([25, 128])\n",
            "torch.Size([31, 128])\n",
            "\n",
            "torch.Size([38, 128])\n",
            "torch.Size([36, 128])\n",
            "\n",
            "torch.Size([25, 128])\n",
            "torch.Size([25, 128])\n",
            "\n",
            "torch.Size([24, 128])\n",
            "torch.Size([25, 128])\n",
            "\n",
            "torch.Size([25, 128])\n",
            "torch.Size([25, 128])\n",
            "\n",
            "torch.Size([27, 128])\n",
            "torch.Size([29, 128])\n",
            "\n",
            "torch.Size([37, 128])\n",
            "torch.Size([40, 128])\n",
            "\n",
            "torch.Size([37, 128])\n",
            "torch.Size([32, 128])\n",
            "\n",
            "torch.Size([32, 128])\n",
            "torch.Size([29, 128])\n",
            "\n",
            "torch.Size([23, 128])\n",
            "torch.Size([25, 128])\n",
            "\n",
            "torch.Size([37, 128])\n",
            "torch.Size([36, 128])\n",
            "\n",
            "torch.Size([30, 128])\n",
            "torch.Size([29, 128])\n",
            "\n",
            "torch.Size([28, 128])\n",
            "torch.Size([29, 128])\n",
            "\n",
            "torch.Size([26, 128])\n",
            "torch.Size([29, 128])\n",
            "\n",
            "torch.Size([29, 128])\n",
            "torch.Size([26, 128])\n",
            "\n",
            "torch.Size([31, 128])\n",
            "torch.Size([34, 128])\n",
            "\n",
            "torch.Size([32, 128])\n",
            "torch.Size([33, 128])\n",
            "\n",
            "torch.Size([25, 128])\n",
            "torch.Size([31, 128])\n",
            "\n",
            "torch.Size([35, 128])\n",
            "torch.Size([33, 128])\n",
            "\n",
            "torch.Size([29, 128])\n",
            "torch.Size([28, 128])\n",
            "\n",
            "torch.Size([29, 128])\n",
            "torch.Size([31, 128])\n",
            "\n",
            "torch.Size([28, 128])\n",
            "torch.Size([32, 128])\n",
            "\n",
            "torch.Size([29, 128])\n",
            "torch.Size([31, 128])\n",
            "\n",
            "torch.Size([30, 128])\n",
            "torch.Size([30, 128])\n",
            "\n",
            "torch.Size([29, 128])\n",
            "torch.Size([28, 128])\n",
            "\n",
            "torch.Size([30, 128])\n",
            "torch.Size([32, 128])\n",
            "\n",
            "torch.Size([38, 128])\n",
            "torch.Size([36, 128])\n",
            "\n",
            "torch.Size([29, 128])\n",
            "torch.Size([24, 128])\n",
            "\n",
            "torch.Size([27, 128])\n",
            "torch.Size([27, 128])\n",
            "\n",
            "torch.Size([26, 128])\n",
            "torch.Size([28, 128])\n",
            "\n",
            "torch.Size([26, 128])\n",
            "torch.Size([25, 128])\n",
            "\n",
            "torch.Size([28, 128])\n",
            "torch.Size([29, 128])\n",
            "\n",
            "torch.Size([27, 128])\n",
            "torch.Size([27, 128])\n",
            "\n",
            "torch.Size([28, 128])\n",
            "torch.Size([28, 128])\n",
            "\n",
            "torch.Size([34, 128])\n",
            "torch.Size([33, 128])\n",
            "\n",
            "torch.Size([34, 128])\n",
            "torch.Size([33, 128])\n",
            "\n",
            "torch.Size([28, 128])\n",
            "torch.Size([28, 128])\n",
            "\n",
            "torch.Size([30, 128])\n",
            "torch.Size([33, 128])\n",
            "\n",
            "torch.Size([27, 128])\n",
            "torch.Size([27, 128])\n",
            "\n",
            "torch.Size([28, 128])\n",
            "torch.Size([28, 128])\n",
            "\n",
            "torch.Size([32, 128])\n",
            "torch.Size([32, 128])\n",
            "\n",
            "torch.Size([29, 128])\n",
            "torch.Size([28, 128])\n",
            "\n",
            "torch.Size([27, 128])\n",
            "torch.Size([28, 128])\n",
            "\n",
            "torch.Size([28, 128])\n",
            "torch.Size([26, 128])\n",
            "\n",
            "torch.Size([30, 128])\n",
            "torch.Size([29, 128])\n",
            "\n",
            "torch.Size([32, 128])\n",
            "torch.Size([32, 128])\n",
            "\n",
            "torch.Size([29, 128])\n",
            "torch.Size([32, 128])\n",
            "\n",
            "torch.Size([26, 128])\n",
            "torch.Size([34, 128])\n",
            "\n",
            "torch.Size([33, 128])\n",
            "torch.Size([28, 128])\n",
            "\n",
            "torch.Size([31, 128])\n",
            "torch.Size([30, 128])\n",
            "\n",
            "torch.Size([45, 128])\n",
            "torch.Size([41, 128])\n",
            "\n",
            "torch.Size([27, 128])\n",
            "torch.Size([31, 128])\n",
            "\n",
            "torch.Size([33, 128])\n",
            "torch.Size([28, 128])\n",
            "\n",
            "torch.Size([27, 128])\n",
            "torch.Size([28, 128])\n",
            "\n",
            "torch.Size([24, 128])\n",
            "torch.Size([24, 128])\n",
            "\n",
            "torch.Size([30, 128])\n",
            "torch.Size([28, 128])\n",
            "\n",
            "torch.Size([34, 128])\n",
            "torch.Size([40, 128])\n",
            "\n",
            "torch.Size([36, 128])\n",
            "torch.Size([31, 128])\n",
            "\n",
            "torch.Size([32, 128])\n",
            "torch.Size([32, 128])\n",
            "\n",
            "torch.Size([31, 128])\n",
            "torch.Size([27, 128])\n",
            "\n",
            "torch.Size([31, 128])\n",
            "torch.Size([33, 128])\n",
            "\n",
            "torch.Size([27, 128])\n",
            "torch.Size([29, 128])\n",
            "\n",
            "torch.Size([24, 128])\n",
            "torch.Size([26, 128])\n",
            "\n",
            "torch.Size([28, 128])\n",
            "torch.Size([29, 128])\n",
            "\n",
            "torch.Size([34, 128])\n",
            "torch.Size([37, 128])\n",
            "\n",
            "torch.Size([34, 128])\n",
            "torch.Size([28, 128])\n",
            "\n",
            "torch.Size([26, 128])\n",
            "torch.Size([26, 128])\n",
            "\n",
            "torch.Size([38, 128])\n",
            "torch.Size([37, 128])\n",
            "\n",
            "torch.Size([27, 128])\n",
            "torch.Size([25, 128])\n",
            "\n",
            "torch.Size([24, 72])\n",
            "torch.Size([26, 72])\n",
            "\n",
            "torch.Size([25, 128])\n",
            "torch.Size([28, 128])\n",
            "\n",
            "torch.Size([36, 128])\n",
            "torch.Size([32, 128])\n",
            "\n",
            "torch.Size([31, 128])\n",
            "torch.Size([26, 128])\n",
            "\n",
            "torch.Size([25, 128])\n",
            "torch.Size([28, 128])\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3aMj1pDXw2l"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}