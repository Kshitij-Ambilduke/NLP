{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seqWattenntion.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNKXKmy9L2o3eSYUBUM9ZMI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kshitij-Ambilduke/NLP/blob/master/seq2seqWithattenntion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaI34MRayYMm"
      },
      "source": [
        "import torch\r\n",
        "from torchtext.datasets import Multi30k\r\n",
        "from torchtext.data import Field, BucketIterator\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.optim as optim\r\n",
        "import spacy\r\n",
        "import numpy as np\r\n",
        "import time"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAniauDS6ZYB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b9d307a-345f-48ea-ab26-01f6237c0a76"
      },
      "source": [
        "!python -m spacy download en --quiet\r\n",
        "!python -m spacy download de --quiet"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "\u001b[K     |████████████████████████████████| 14.9MB 5.1MB/s \n",
            "\u001b[?25h  Building wheel for de-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/de_core_news_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/de\n",
            "You can now load the model via spacy.load('de')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s085aSpmgz8v"
      },
      "source": [
        "spacy_german = spacy.load(\"de\")\r\n",
        "spacy_english = spacy.load(\"en\")\r\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vy6BPMiGYY-s"
      },
      "source": [
        "device = 'cuda'"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJJTZ596g31h"
      },
      "source": [
        "def en_tokenizer(sen):\r\n",
        "    tokens = []\r\n",
        "    for token in spacy_english.tokenizer(sen):\r\n",
        "        tokens.append(token.text)\r\n",
        "    return tokens\r\n",
        "\r\n",
        "def de_tokenizer(sen):\r\n",
        "    tokens = []\r\n",
        "    for token in spacy_german.tokenizer(sen):\r\n",
        "        tokens.append(token.text)\r\n",
        "    return tokens\r\n",
        "      "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-ku7mFchB6U"
      },
      "source": [
        "SOURCE_Field = Field(eos_token = '<src_eos>', init_token = '<src_sos>', lower = True, tokenize = de_tokenizer)\r\n",
        "TARGET_Field = Field(eos_token = '<trg_eos>', init_token = '<trg_sos>', lower = True, tokenize = en_tokenizer)\r\n",
        "\r\n",
        "train_data, valid_data, test_data = Multi30k.splits(exts = (\".de\", \".en\"),fields=(SOURCE_Field, TARGET_Field))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewtyGGpA9OAR",
        "outputId": "3c28a1e0-5f3c-49aa-d4b2-91846bf2f114"
      },
      "source": [
        "print(vars(train_data[0]))"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'src': ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.'], 'trg': ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JirhH6QghD_k"
      },
      "source": [
        "SOURCE_Field.build_vocab(train_data, min_freq=2)\r\n",
        "TARGET_Field.build_vocab(train_data, min_freq=2)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HBoW0vshOan"
      },
      "source": [
        "BATCH_SIZE = 128\r\n",
        "\r\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\r\n",
        "    (train_data, valid_data, test_data), \r\n",
        "    batch_size = BATCH_SIZE, device = device)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kL3WNNCLhSHV"
      },
      "source": [
        "class Encoder(nn.Module):\r\n",
        "    def __init__(self, source_vocab_len, embeddingsize, hiddensize, batchsize):\r\n",
        "        super().__init__()\r\n",
        "        self.batch_size = batchsize\r\n",
        "        self.embed = nn.Embedding(source_vocab_len, embeddingsize)\r\n",
        "        self.lstm = nn.LSTM(embeddingsize, hiddensize,num_layers=4,bidirectional = True)\r\n",
        "        self.fc = nn.Linear(hiddensize,hiddensize)\r\n",
        "        \r\n",
        "    \r\n",
        "    def forward(self, x):\r\n",
        "        x = self.embed(x)              # input = (seqlen, batch), output = (seqlen, batch, embedding_dim)\r\n",
        "        op , (h,c) = self.lstm(x)      # input(x)=(seqlen,batch,embedding_dim), \r\n",
        "        h = torch.tanh(h)              # h = (1, batch, hidden_size)\r\n",
        "        c = torch.tanh(c)\r\n",
        "        op = self.fc(op)\r\n",
        "        \r\n",
        "        return op,h,c"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYtqYUt3k5v0"
      },
      "source": [
        "class Decoder(nn.Module):\r\n",
        "    def __init__(self, target_vocab_len, embeddingsize, hiddensize):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        self.opsize = target_vocab_len\r\n",
        "        self.embed = nn.Embedding(target_vocab_len,embeddingsize)\r\n",
        "        self.lstm = nn.LSTM(embeddingsize, hiddensize,num_layers=4)\r\n",
        "        self.fc = nn.Linear(2*hiddensize, self.opsize)\r\n",
        "        self.smax = nn.Softmax()\r\n",
        "\r\n",
        "    def forward(self, x, h0, c0, encoder_op):\r\n",
        "\r\n",
        "        x = self.embed(x)                 # input    = [1, batch], output = [1, batch, embedding_dim]      \r\n",
        "        op, (h,c) = self.lstm(x,(h0,c0))  # input(x) = [1, batch, embedding_dim], op = [1, batch, hidden_dim] \r\n",
        "        op = torch.tanh(op)\r\n",
        "        # print(encoder_op.shape)\r\n",
        "        # print(op.shape)\r\n",
        "        at = torch.mul(encoder_op,op )\r\n",
        "        at = torch.sum(op, dim=1)\r\n",
        "        at = at.squeeze()\r\n",
        "        # print(at.shape)\r\n",
        "        at = self.smax(at)\r\n",
        "        at = torch.mul(at,op)\r\n",
        "        at = torch.sum(at,dim=0) #at = [128,128]\r\n",
        "        at = at.unsqueeze(0)  #at = [1,128,128]\r\n",
        "        # print(at.shape)\r\n",
        "        # print(op.shape)\r\n",
        "        op = torch.cat((at,op),dim=2)\r\n",
        "        op = self.fc(op)                  # op = [1, batch, vocabsize] (vocabsize==output_size)\r\n",
        "\r\n",
        "        return op,(h,c)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Y41-jocZQgN"
      },
      "source": [
        "class Translator(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self,encoder, decoder):\r\n",
        "        super().__init__()\r\n",
        "        self.encoder = encoder\r\n",
        "        self.decoder = decoder\r\n",
        "    \r\n",
        "    def forward(self, source, target):\r\n",
        "        \r\n",
        "        #source = [seq len, batch size]\r\n",
        "        #target = [seq len, batch size]\r\n",
        "\r\n",
        "        target_len = target.shape[0]\r\n",
        "        seq_len = target.shape[0]\r\n",
        "        vocab_len = self.decoder.opsize\r\n",
        "        batch_size = source.shape[-1]\r\n",
        "\r\n",
        "        enc_op, hid_state, cell_state = self.encoder(source) \r\n",
        "\r\n",
        "        inp = target[0,:]                 #As input to the decoder is start token  #here, inp = [batch_size]\r\n",
        "        inp = inp.unsqueeze(0)                                                     #here, inp = [1,batch_size]\r\n",
        "        prediction = torch.zeros(target_len, batch_size, vocab_len).to(device)\r\n",
        "        for i in range(1, target_len):\r\n",
        "            batch_size = source.shape[-1]\r\n",
        "            output,state = self.decoder(inp, hid_state, cell_state, enc_op) #output = [1, batch, vocabsize]\r\n",
        "            # print(output.shape)\r\n",
        "            hid_state, cell_state = state\r\n",
        "            prediction[i] = output.view(batch_size,self.decoder.opsize)\r\n",
        "            inp = target[i].unsqueeze(0)\r\n",
        "        # print(\"modelreturn=\",prediction.shape)            \r\n",
        "        return prediction\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruE3Dy09Zo-7"
      },
      "source": [
        "source_vocab_len = len(SOURCE_Field.vocab)\r\n",
        "target_vocab_len = len(TARGET_Field.vocab)\r\n",
        "encoder_embedding_dim = 64\r\n",
        "decoder_embedding_dim = 64\r\n",
        "hiddensize_encoder = 128\r\n",
        "hiddensize_decoder = 128\r\n",
        "enc = Encoder(source_vocab_len, encoder_embedding_dim, hiddensize_encoder, 128)\r\n",
        "dec = Decoder(target_vocab_len, decoder_embedding_dim, hiddensize_decoder )\r\n",
        "model = Translator(enc,dec).to(device)\r\n",
        "optimizer = optim.Adam(model.parameters(),lr=0.001)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5sG2YS7ZyG2",
        "outputId": "60a36886-a44b-4dd2-e098-79bd425fd304"
      },
      "source": [
        "print(model)\r\n",
        "def count_parameters(model):\r\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n",
        "\r\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Translator(\n",
            "  (encoder): Encoder(\n",
            "    (embed): Embedding(7855, 64)\n",
            "    (lstm): LSTM(64, 128, num_layers=4)\n",
            "    (fc): Linear(in_features=128, out_features=128, bias=True)\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (embed): Embedding(5893, 64)\n",
            "    (lstm): LSTM(64, 128, num_layers=4)\n",
            "    (fc): Linear(in_features=256, out_features=5893, bias=True)\n",
            "    (smax): Softmax(dim=None)\n",
            "  )\n",
            ")\n",
            "The model has 3,402,117 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDUnPA_WZ8uN"
      },
      "source": [
        "#Our loss function calculates the average loss per token, however by passing the index of the <pad> token as \r\n",
        "#the ignore_index argument we ignore the loss whenever the target token is a padding token.\r\n",
        "target_padding_index = TARGET_Field.vocab.stoi[TARGET_Field.pad_token]\r\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = target_padding_index)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEMr729JaAFD"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\r\n",
        "  model.train()                               #just tells pytorch that we are in training phase\r\n",
        "  epoch_loss = 0\r\n",
        "  for i, batch in enumerate(train_iterator):\r\n",
        "      source = batch.src\r\n",
        "      target = batch.trg\r\n",
        "      # print(\"source=\",source.shape,\"target=\",target.shape)\r\n",
        "      # print(\"t=\",target.shape)\r\n",
        "      optimizer.zero_grad()\r\n",
        "      output = model.forward(source, target)  #target = [trg len, batch size]\r\n",
        "                                              #output = [trg len, batch size, output dim]\r\n",
        "      output_dim = output.shape[-1]\r\n",
        "      # print(\"op=\",output.shape)\r\n",
        "      output = output[1:].view(-1,output_dim)\r\n",
        "      target = target[1:].view(-1)\r\n",
        "      # print(\"op2=\",output.shape)\r\n",
        "      # print(\"t=\",target.shape)\r\n",
        "      loss = criterion(output, target)\r\n",
        "      \r\n",
        "      loss.backward()\r\n",
        "      optimizer.step()\r\n",
        "      epoch_loss += loss.item()\r\n",
        "\r\n",
        "  return epoch_loss / len(iterator)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rN9D2fhtaHFu",
        "outputId": "76f0f7f3-5571-427b-ce4e-1e3650d59196"
      },
      "source": [
        "for epoch in range(10):\r\n",
        "    start = time.time()\r\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion)\r\n",
        "    end = time.time()\r\n",
        "    print(train_loss,end='|')\r\n",
        "    print(\" time taken = \",end-start)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2.823803711567681| time taken =  25.852331399917603\n",
            "2.783943460901403| time taken =  25.938648223876953\n",
            "2.7444552198905776| time taken =  25.701021909713745\n",
            "2.7068162090452756| time taken =  25.7673499584198\n",
            "2.669777913240609| time taken =  25.681989192962646\n",
            "2.635799615918802| time taken =  25.744008541107178\n",
            "2.6033190357527545| time taken =  25.741931915283203\n",
            "2.5701955625139146| time taken =  25.73134708404541\n",
            "2.5383616313010062| time taken =  25.887011289596558\n",
            "2.508759267529727| time taken =  25.52130389213562\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoCObVxeizKI"
      },
      "source": [
        "def ipTensor(sentence, src_field):\r\n",
        "    if isinstance(sentence, list):\r\n",
        "        tokens = [src_field.init_token] + [token.lower() for token in sentence] + [src_field.eos_token]\r\n",
        "    else:\r\n",
        "        tokens = [src_field.init_token] + de_tokenizer(sentence) + [src_field.eos_token]\r\n",
        "    seq_len = len(tokens)\r\n",
        "    ip_tensor = torch.LongTensor([src_field.vocab.stoi[token] for token in tokens])\r\n",
        "    return ip_tensor.view(seq_len, 1)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8656nKquC4q"
      },
      "source": [
        "def converter(source_sen, source_field, target_field, model):\r\n",
        "    input_tensor = ipTensor(source_sen, source_field)\r\n",
        "    input_tensor=input_tensor.to(device)\r\n",
        "    with torch.no_grad():\r\n",
        "        op,h,c = model.encoder(input_tensor)\r\n",
        "        states = (h,c)\r\n",
        "    sos_loc = target_field.vocab.stoi[target_field.init_token]\r\n",
        "    eos_loc = target_field.vocab.stoi[target_field.eos_token]\r\n",
        "    predicts = [sos_loc]\r\n",
        "    sen_len =1\r\n",
        "    while sen_len < 50:\r\n",
        "        inp = torch.LongTensor([predicts[-1]]).view(1,-1)\r\n",
        "        with torch.no_grad():\r\n",
        "            h,c=states\r\n",
        "            inp = inp.to(device)\r\n",
        "            h = h.to(device)\r\n",
        "            c = c.to(device)\r\n",
        "            op = op.to(device)\r\n",
        "            output, states = model.decoder(inp, h,c,op)\r\n",
        "        output = output.squeeze()\r\n",
        "        output = output.view(-1,model.decoder.opsize)\r\n",
        "        predicts.append(output.argmax(-1).item())\r\n",
        "        sen_len+=1\r\n",
        "        if predicts[-1]==eos_loc:\r\n",
        "            break\r\n",
        "    sentence = [target_field.vocab.itos[it] for it in predicts[1:]]\r\n",
        "    return sentence"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsF9tk0tujSp",
        "outputId": "7fc5f7a0-cede-4f56-f40f-f9da620bcdb8"
      },
      "source": [
        "sen = ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.']\r\n",
        "output = converter(sen, SOURCE_Field, TARGET_Field, model)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1-7oqZqlqUi",
        "outputId": "5f07cc7e-0ae9-4374-8cf5-bcda7c872e48"
      },
      "source": [
        "print(output)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['two', 'young', 'young', 'a', 'white', 'soccer', 'players', 'are', 'running', 'in', 'the', 'grass', '.', '<trg_eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sh9l0MVPU9PU"
      },
      "source": [
        "['two', 'men', 'are', 'sitting', 'on', 'a', 'street', '.', '<trg_eos>']\r\n",
        "['two', 'young', 'men', 'are', 'walking', 'in', 'the', 'water', '.', '<trg_eos>']\r\n",
        "['two', 'young', 'young', 'a', 'white', 'soccer', 'players', 'are', 'running', 'in', 'the', 'grass', '.', '<trg_eos>']"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}